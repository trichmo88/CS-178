{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 178 Final",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO7yPpj6REiH"
      },
      "source": [
        "# Settings\n",
        "NN_USE_LAST = True\n",
        "PTH_PATH = '/content/drive/My Drive/CS 178/net.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM5VTCGJctFI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ITsbzUeSkqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6740dcb9-9672-4873-cb7d-0f7d2c484d31"
      },
      "source": [
        "# Use gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Add path\n",
        "import sys\n",
        "sys.path.insert(1, '/content/drive/My Drive/CS 178')\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import mltools as ml\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.utils.estimator_checks import check_estimator\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmubiFYqKuEy"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2feGzg_KxUR"
      },
      "source": [
        "Each classifier must have these functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrc8YxV4_QEw"
      },
      "source": [
        "class Classifier():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def train(self, X, Y):\n",
        "    pass\n",
        "\n",
        "  def predict(self, X):\n",
        "    pass\n",
        "\n",
        "  def auc(self, Y, Ypred):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(Y, Ypred[:, 1])\n",
        "    return metrics.auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjHGnFYpM5J3"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH0B_qTSNWt3"
      },
      "source": [
        "class RFClassifier(Classifier):\n",
        "  def __init__(self, nBags, maxDepth, minParent):\n",
        "    super().__init__()\n",
        "    self.nBags = nBags\n",
        "    self.maxDepth = maxDepth\n",
        "    self.minParent = minParent\n",
        "    self.classifiers = [None] * nBags\n",
        "\n",
        "  def train(self, X, Y):\n",
        "    m = X.shape[0]\n",
        "    for i in range(self.nBags):\n",
        "      Xi, Yi = ml.bootstrapData(X, Y, m)\n",
        "      self.classifiers[i] = ml.dtree.treeClassify(Xi, Yi, maxDepth=self.maxDepth, minParent=self.minParent)\n",
        "\n",
        "  def predict(self, X):\n",
        "    predict = [self.classifiers[i].predictSoft(X) for i in range(self.nBags)]\n",
        "    return np.mean(predict, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjswKU7dLAuF"
      },
      "source": [
        "## kNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh93R4v5LC0A"
      },
      "source": [
        "class KNNClassifier(Classifier):\n",
        "  def __init__(self, n_neighbors, weights):\n",
        "    super().__init__()\n",
        "    self.classifier = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
        "\n",
        "  def train(self, X, Y):\n",
        "    self.classifier = self.classifier.fit(X,Y)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return self.classifier.predict_proba(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg3QcS6iK0kr"
      },
      "source": [
        "## Neural Network Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwBrqaReFxTv"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, features, layers):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    # Assemble layers.\n",
        "    all_layers = []\n",
        "    input_size = features\n",
        "    for i in layers[:-1]:\n",
        "      all_layers.append(nn.Linear(input_size, i))\n",
        "      all_layers.append(nn.Tanh())\n",
        "      # all_layers.append(nn.BatchNorm1d(i))\n",
        "      # all_layers.append(nn.Dropout(0.2))\n",
        "      input_size = i\n",
        "\n",
        "    all_layers.append(nn.Linear(input_size, layers[-1]))\n",
        "    all_layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "    self.layers = nn.Sequential(*all_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layers(x)\n",
        "    return x\n",
        "\n",
        "class NNClassifier(Classifier):\n",
        "  def __init__(self, n_features, layers, output, class_weights, pth=None):\n",
        "    super().__init__()\n",
        "    n_l = [n_features * l for l in layers]\n",
        "    n_l.append(output)\n",
        "    self.net = Net(n_features, n_l)\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.net.to(self.device)\n",
        "    self.weights = torch.Tensor(class_weights).float().to(self.device)\n",
        "    self.criterion = nn.CrossEntropyLoss(weight=self.weights)\n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.001)\n",
        "\n",
        "    self.pth = pth\n",
        "    if self.pth:\n",
        "      self.net.load_state_dict(torch.load(self.pth))\n",
        "\n",
        "  def train(self, X, Y, epochs=2000, batch_size=10000):\n",
        "    if self.pth:\n",
        "      return\n",
        "\n",
        "    X = torch.from_numpy(X).float()\n",
        "    Y = torch.from_numpy(Y).float().type(torch.LongTensor)\n",
        "\n",
        "    losses = []\n",
        "    timer = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      epoch_loss = 0\n",
        "      start = 0\n",
        "      end = batch_size\n",
        "      for i in range(int(X.shape[0] / batch_size)):\n",
        "        Xb = X[start:end]\n",
        "        Yb = Y[start:end]\n",
        "        start += batch_size\n",
        "        end += batch_size\n",
        "\n",
        "        input = Xb.to(self.device)\n",
        "        output = Yb.to(self.device)\n",
        "        yhat = self.net(input)\n",
        "        loss = self.criterion(yhat, output)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      losses.append(epoch_loss)\n",
        "      if epoch % 100 == 0:\n",
        "        print(f\"[{epoch}] {loss.item():10.8f}\")\n",
        "\n",
        "    print(f\"[{epochs}] {loss.item():10.8f}\")\n",
        "    print('Finished training in', str(time.time() - timer) + 's')\n",
        "\n",
        "  def predict(self, X):\n",
        "    with torch.no_grad():\n",
        "      input = torch.from_numpy(X).float().to(self.device)\n",
        "      yhat = self.net(input)\n",
        "      return yhat.cpu().numpy()\n",
        "\n",
        "  def loss(self, X, Y):\n",
        "    with torch.no_grad():\n",
        "      input = torch.from_numpy(X).float().to(self.device)\n",
        "      output = torch.from_numpy(Y).float().type(torch.LongTensor).to(self.device)\n",
        "      yhat = self.net(input)\n",
        "      loss = self.criterion(yhat, output)\n",
        "      return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvcs3OlzPQX-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XI1LtnP_bz"
      },
      "source": [
        "Load training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDVTu2uEP-e8"
      },
      "source": [
        "X = np.genfromtxt('/content/drive/My Drive/CS 178/X_train.txt', delimiter=None)\n",
        "Y = np.genfromtxt('/content/drive/My Drive/CS 178/Y_train.txt', delimiter=None)\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "Xtr, Xva, Ytr, Yva = train_test_split(X, Y, test_size=.25, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ9sx0cpPTNi"
      },
      "source": [
        "Create classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RReBkYt-PjKk"
      },
      "source": [
        "# Random Forest\n",
        "rf = RFClassifier(100, 4, 1024)\n",
        "\n",
        "# kNN\n",
        "knn = KNNClassifier(10, 'distance')\n",
        "\n",
        "# Neural Network\n",
        "classes = np.unique(Y)\n",
        "n_i = X.shape[1]\n",
        "n_o = len(classes)\n",
        "classes_size = np.zeros(len(classes))\n",
        "for i, c in enumerate(classes):\n",
        "  classes_size[i] = (Y == c).sum().item()\n",
        "weights = [1 / c for c in classes_size]\n",
        "pth = PTH_PATH if NN_USE_LAST else None\n",
        "nn = NNClassifier(n_i, [64, 32, 16, 8], n_o, weights, pth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1DscOszTwMV"
      },
      "source": [
        "Train each classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FygyX0H2TifJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ebae42c8-a124-4ce4-8164-62310d8fffc6"
      },
      "source": [
        "print('Training random forest...')\n",
        "rf.train(Xtr, Ytr)\n",
        "\n",
        "print('Training kNN...')\n",
        "knn.train(Xtr, Ytr)\n",
        "\n",
        "print('Training neural network...')\n",
        "nn.train(Xtr, Ytr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training random forest...\n",
            "Training kNN...\n",
            "Training neural network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AJHTNkZV3t1"
      },
      "source": [
        "Get prediction for each classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOkYDFAnV5cG"
      },
      "source": [
        "predRFtr = rf.predict(Xtr)\n",
        "predRFva = rf.predict(Xva)\n",
        "\n",
        "predKNNtr = knn.predict(Xtr)\n",
        "predKNNva = knn.predict(Xva)\n",
        "\n",
        "predNNtr = nn.predict(Xtr)\n",
        "predNNva = nn.predict(Xva)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPS-rYzpUGVh"
      },
      "source": [
        "Print each classifier score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6lPgtv7UF6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "d21a708e-9e99-410b-ec3a-e19032e20230"
      },
      "source": [
        "print(\"TRAINING SCORE\")\n",
        "print('Random forest:', rf.auc(Ytr, predRFtr))\n",
        "print('kNN:', knn.auc(Ytr, predKNNtr))\n",
        "print('NN:', nn.auc(Ytr, predNNtr))\n",
        "print()\n",
        "print(\"VALIDATION SCORE\")\n",
        "print('Random forest:', rf.auc(Yva, predRFva))\n",
        "print('kNN:', knn.auc(Yva, predKNNva))\n",
        "print('NN:', nn.auc(Yva, predNNva))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING SCORE\n",
            "Random forest: 0.6649325596032527\n",
            "kNN: 0.9941760208037207\n",
            "NN: 0.8997748078966752\n",
            "\n",
            "VALIDATION SCORE\n",
            "Random forest: 0.6584797879452038\n",
            "kNN: 0.716388364479713\n",
            "NN: 0.898101000076494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBsmpQFCbUsL"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArwMIaTcTdU"
      },
      "source": [
        "def auc(Y, Ypred):\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(Y, Ypred)\n",
        "  return metrics.auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr5XsS2ibZjI"
      },
      "source": [
        "## Majority Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKhiWtqTWi9I"
      },
      "source": [
        "def majorityVote(Ypred, weights, size):\n",
        "  # Ypred - list of predictions per classifer\n",
        "  # weights - list of weights per each classifer\n",
        "  # size - size of dataset\n",
        "  \n",
        "  Yfin = np.zeros(size)\n",
        "\n",
        "  for i in range(size):\n",
        "    yi = [y[i, 1] for y in Ypred]\n",
        "    pred_0, pred_1 = 0, 0\n",
        "    predv_0, predv_1 = 0, 0\n",
        "    for j, yij in enumerate(yi):\n",
        "      if yij < 0.5:\n",
        "        pred_0 += weights[j]\n",
        "        predv_0 += yij * weights[j]\n",
        "      else:\n",
        "        pred_1 += weights[j]\n",
        "        predv_1 += yij * weights[j]\n",
        "\n",
        "    if pred_0 > pred_1:\n",
        "      pred_final = predv_0 / pred_0\n",
        "    else:\n",
        "      pred_final = predv_1 / pred_1\n",
        "\n",
        "    Yfin[i] = pred_final\n",
        "\n",
        "  return Yfin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aemnLoaHb3rn"
      },
      "source": [
        "Test out some combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJk7zvfib5lA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "23046442-6939-4810-d820-6c4e9dc10d08"
      },
      "source": [
        "# KNN VAL: predVal, predTrain \n",
        "# RF: predictVa, predictTr\n",
        "# NN: YvaPredNN, YtrPredNN\n",
        "\n",
        "weights = [rf.auc(Yva, predRFva), knn.auc(Yva, predKNNva), nn.auc(Yva, predNNva)]\n",
        "\n",
        "print('RF + KNN')\n",
        "# Result of RF + KNN on Training\n",
        "ensembleTr = majorityVote([predRFtr, predKNNtr], weights, len(Ytr))\n",
        "print('Training:', auc(Ytr, ensembleTr))\n",
        "\n",
        "# Results of RF + KNN on Validation\n",
        "ensembleVa = majorityVote([predRFva, predKNNva], weights, len(Yva))\n",
        "print('Validation:', auc(Yva, ensembleVa))\n",
        "\n",
        "print()\n",
        "print('RF + KNN + NN')\n",
        "# Result of RF + KNN + NN on Training\n",
        "Ypredtr = [predRFtr, predKNNtr, predNNtr]\n",
        "ensembleTr = majorityVote(Ypredtr, weights, len(Ytr))\n",
        "print('Training:', auc(Ytr, ensembleTr))\n",
        "\n",
        "# Results of RF + KNN + NN on Validation\n",
        "YpredVa = [predRFva, predKNNva, predNNva]\n",
        "ensembleVa = majorityVote(YpredVa, weights, len(Yva))\n",
        "print('Validation:', auc(Yva, ensembleVa))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF + KNN\n",
            "Training: 0.9940122971370583\n",
            "Validation: 0.7182815038507545\n",
            "\n",
            "RF + KNN + NN\n",
            "Training: 0.9753933073436409\n",
            "Validation: 0.8408107774566507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soNQ0gKA1jMQ"
      },
      "source": [
        "Xte = np.genfromtxt('/content/drive/My Drive/CS 178/X_test.txt', delimiter=None)\n",
        "Xte = scaler.transform(Xte)\n",
        "Ypred = [rf.predict(Xte), knn.predict(Xte), nn.predict(Xte)]\n",
        "Yte = majorityVote(Ypred, weights, Xte.shape[0])\n",
        "Yte = np.vstack((np.arange(Xte.shape[0]), Yte)).T\n",
        "np.savetxt('/content/drive/My Drive/CS 178/Y_Predictions.txt', Yte, '%d, %.2f', header='ID,Prob1', comments='', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNEUxESS-b6h"
      },
      "source": [
        "## Majority Voting 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOmVxHZM-enu"
      },
      "source": [
        "This one use the idea from [here](https://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QiviHmN-rQp"
      },
      "source": [
        "def majorityVote2(Ypred, weights):\n",
        "  # Ypred - list of predictions per classifer (np.array)\n",
        "  # weights - list of weights per each classifer\n",
        "  \n",
        "  n_classifier, n_data, n_class = Ypred.shape\n",
        "  Yfin = [None] * n_data\n",
        "\n",
        "  for i in range(n_data):\n",
        "    yi = Ypred[:, i, :]\n",
        "    d_class = np.zeros(n_class)\n",
        "    for j in range(n_class):\n",
        "      d_class[j] = np.sum(weights * yi[:, j]) / n_classifier\n",
        "    Yfin[i] = d_class\n",
        "  return np.array(Yfin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1_PcxyP_EfM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fd71e638-b16b-45b0-a1b7-0cc328966baf"
      },
      "source": [
        "aucs = [rf.auc(Yva, predRFva), knn.auc(Yva, predKNNva), nn.auc(Yva, predNNva)]\n",
        "weights = [max(aucs) / auc for auc in aucs]\n",
        "\n",
        "print('RF + KNN + NN')\n",
        "# Result of RF + KNN + NN on Training\n",
        "YpredTr = np.array([predRFtr, predKNNtr, predNNtr])\n",
        "ensembleTr = majorityVote2(YpredTr, weights)\n",
        "print('Training:', auc(Ytr, ensembleTr[:, 1]))\n",
        "\n",
        "# Results of RF + KNN + NN on Validation\n",
        "YpredVa = np.array([predRFva, predKNNva, predNNva])\n",
        "ensembleVa = majorityVote2(YpredVa, weights)\n",
        "print('Validation:', auc(Yva, ensembleVa[:, 1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF + KNN + NN\n",
            "Training: 0.9856361422101106\n",
            "Validation: 0.8731616079713447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibRTtPhb-pCa"
      },
      "source": [
        "Xte = np.genfromtxt('/content/drive/My Drive/CS 178/X_test.txt', delimiter=None)\n",
        "Xte = scaler.transform(Xte)\n",
        "Ypred = [rf.predict(Xte), knn.predict(Xte), nn.predict(Xte)]\n",
        "Yte = majorityVote(Ypred, weights, Xte.shape[0])\n",
        "Yte = np.vstack((np.arange(Xte.shape[0]), Yte)).T\n",
        "np.savetxt('/content/drive/My Drive/CS 178/Y_Predictions.txt', Yte, '%d, %.2f', header='ID,Prob1', comments='', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzVoZ4KkgjbC"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71mYPsMaI7VA"
      },
      "source": [
        "Create stacking classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTxzoPByI8yZ"
      },
      "source": [
        "estimators = [\n",
        "              ('knn', KNeighborsClassifier(n_neighbors=10)), \n",
        "              ('rf', RandomForestClassifier(max_depth=1024)), \n",
        "              ('nn', MLPClassifier(hidden_layer_sizes=(8, 4), activation='tanh'))\n",
        "             ]\n",
        "sk = StackingClassifier(estimators=estimators, stack_method='predict_proba')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCy-uP55SGmH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "e874764a-8e2b-48e5-b716-35682d9633e3"
      },
      "source": [
        "sk.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('knn',\n",
              "                                KNeighborsClassifier(algorithm='auto',\n",
              "                                                     leaf_size=30,\n",
              "                                                     metric='minkowski',\n",
              "                                                     metric_params=None,\n",
              "                                                     n_jobs=None,\n",
              "                                                     n_neighbors=10, p=2,\n",
              "                                                     weights='uniform')),\n",
              "                               ('rf',\n",
              "                                RandomForestClassifier(bootstrap=True,\n",
              "                                                       ccp_alpha=0.0,\n",
              "                                                       class_weight=None,\n",
              "                                                       criterion='gini',\n",
              "                                                       max_depth=1024,\n",
              "                                                       max_features='auto',\n",
              "                                                       max_leaf_nodes=None,\n",
              "                                                       max_samples=None...\n",
              "                                              learning_rate='constant',\n",
              "                                              learning_rate_init=0.001,\n",
              "                                              max_fun=15000, max_iter=200,\n",
              "                                              momentum=0.9, n_iter_no_change=10,\n",
              "                                              nesterovs_momentum=True,\n",
              "                                              power_t=0.5, random_state=None,\n",
              "                                              shuffle=True, solver='adam',\n",
              "                                              tol=0.0001,\n",
              "                                              validation_fraction=0.1,\n",
              "                                              verbose=False,\n",
              "                                              warm_start=False))],\n",
              "                   final_estimator=None, n_jobs=None, passthrough=False,\n",
              "                   stack_method='predict_proba', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcJN49cRSJBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b66b7423-c1cb-438d-8e97-020cc80cdadb"
      },
      "source": [
        "yhat = sk.predict_proba(Xtr)\n",
        "print('Stack Training:', auc(Ytr, yhat[:, 1]))\n",
        "\n",
        "yhat = sk.predict_proba(Xva)\n",
        "print('Stack Validation:', auc(Yva, yhat[:, 1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stack Training: 0.9699776811410254\n",
            "Stack Validation: 0.9696968496017537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQt1jxcsowYd"
      },
      "source": [
        "Xte = np.genfromtxt('/content/drive/My Drive/CS 178/X_test.txt', delimiter=None)\n",
        "Xte = scaler.transform(Xte)\n",
        "Yte = sk.predict_proba(Xte)\n",
        "Yte = np.vstack((np.arange(Xte.shape[0]), Yte[:, 1])).T\n",
        "np.savetxt('/content/drive/My Drive/CS 178/Y_PredictionsStack.txt', Yte, '%d, %.2f', header='ID,Prob1', comments='', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}